# Chapter 26 - Deep Reinforence Learningï¼ˆPart 4 - Asynchronous Advantage Actor-Criticï¼ˆA3Cï¼‰ï¼‰

[1.RLä¸­å­˜åœ¨çš„é—®é¢˜ä¸A3Cçš„æå‡º](#1)

â€‹		[1.1 ä½¿ç”¨é•¿æœŸæ”¶ç›ŠæœŸæœ›è§£å†³å³æ—¶æ”¶ç›Šçš„ä¸ç¨³å®šé—®é¢˜](#1.1)

â€‹		[1.2 Asynchronous Advantage Actor-Criticï¼ˆA3Cï¼‰](#1.2)

[2.Pathwise Derivative Policy Gradient](#2)

â€‹		[2.1 å€Ÿé‰´GANçš„æ€æƒ³ä½¿ç”¨Actorè§£å†³Q-Learningçš„arg maxé—®é¢˜](#2.1)

â€‹		[2.2 Pathwise Derivative Policy Gradientç®—æ³•ä¼ªä»£ç ](#2.2)



#### Abstractï¼šA3Cæ˜¯Actor-Criticæ–¹æ³•ä¸­æœ€çŸ¥åçš„ä¸€ç§ï¼Œå…¶åŸå§‹æ–‡ç« ä¸ºï¼š[Volodymyr Mnih, AdriaÌ€ PuigdomeÌ€nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, â€œAsynchronous Methods for Deep Reinforcement Learningâ€, ICML, 2016]



#### <span name="1">1.RLä¸­å­˜åœ¨çš„é—®é¢˜ä¸A3Cçš„æå‡º</span>

1. <span name="1.1">ä½¿ç”¨é•¿æœŸæ”¶ç›ŠæœŸæœ›è§£å†³å³æ—¶æ”¶ç›Šçš„ä¸ç¨³å®šé—®é¢˜</span>

   - Policy Gradient Reviewï¼šPolicy Gradientçš„æ›´æ–°é‡ä¸º$\nabla\tilde{R}_\theta$å¦‚ä¸‹ã€‚ä»¤$G_t^n=\sum\limits_{t'=t}^{T_n} \gamma^{t'-t} r_{t'}^n$è¡¨ç¤ºCumulated Rewardã€‚å®é™…ä¸Šç”±äºäº’åŠ¨è¿‡ç¨‹å­˜åœ¨ä¸€å®šçš„éšæœºæ€§ï¼Œ$G_t^n$æ˜¯ä¸€ä¸ªRandom Variableã€‚æˆ‘ä»¬æ˜¯é€šè¿‡é‡‡æ ·çš„æ–¹å¼è®¡ç®—$G_t^n$çš„å€¼ï¼Œæ‰€ä»¥ç»“æœå¾ˆä¸ç¨³å®šçš„ï¼Œ$G_t^n$å°±å˜æˆä¸€ä¸ªä¸€ä¸ªVarianceå¾ˆå¤§çš„Random Variableã€‚åªæœ‰åœ¨é‡‡æ ·çš„æ•°æ®è¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½å‡†ç¡®ä¼°è®¡$G_t^n$ï¼Œä½†è¿™åœ¨å®é™…è®­ç»ƒæ—¶æ˜¯ä¸å¯èƒ½çš„ã€‚æ‰€ä»¥æå‡ºä¸€ç§æƒ³æ³•ï¼Œè®­ç»ƒä¸€ä¸ªç½‘ç»œå»ä¼°è®¡$G_t^n$çš„æœŸæœ›å€¼ã€‚

     <img src="./image-20200826202825240.png" alt="image-20200826202825240" style="zoom: 50%;" />
     
   - Q-Learning Reviewï¼š
   
     <img src="./image-20200826203932703.png" alt="image-20200826203932703" style="zoom:50%;" />
     
   - Actor-Criticï¼šä½¿ç”¨Q-Learningçš„æ–¹æ³•è®¡ç®—Policy Gradientä¸­ç´¯è®¡æ”¶ç›Šçš„æœŸæœ›å€¼ã€‚$Q^{\pi_\theta}(s_t^n,a_t^n)$çš„å®šä¹‰æ°å¥½å°±æ˜¯Cumulated Reward Expectï¼Œæ‰€ä»¥ä½¿ç”¨$Q^{\pi_\theta}(s_t^n,a_t^n)$ä»£æ›¿$G_t^n$ã€‚å› ä¸ºåœ¨State $s$æ—¶ï¼Œ$V^{\pi_\theta}(s_t^n)$æ˜¯æ²¡æœ‰å¼•å…¥Actionçš„Cumulated Reward Expectï¼Œ$Q^{\pi_\theta}(s_t^n,a_t^n)$æ˜¯å¼•å…¥äº†æŒ‡å®šActionçš„Cumulated Reward Expectï¼Œæ‰€ä»¥$V^{\pi_\theta}(s_t^n)$å…¶å®æ˜¯$Q^{\pi_\theta}(s_t^n,a_t^n)$çš„æœŸæœ›å€¼ï¼Œæ‰€ä»¥ä½¿ç”¨$V^{\pi_\theta}(s_t^n)$ä»£æ›¿Baseline $b$ã€‚$Q^{\pi_\theta}(s_t^n,a_t^n)-V^{\pi_\theta}(s_t^n)$å°±ä¼šå˜æˆä¸€ä¸ªå¯æ­£å¯è´Ÿçš„å€¼ã€‚
   
     <img src="./image-20200826204751314.png" alt="image-20200826204751314" style="zoom: 50%;" />
     
   - åŸºäºä¸Šè¿°å…¬å¼ï¼Œå°±å¯ä»¥è¿›è¡Œä»£ç å®ç°ï¼Œä½†æ˜¯éœ€è¦ä¸¤ä¸ªç¥ç»ç½‘ç»œåˆ†åˆ«å¯¹$Q,V$è¿›è¡Œä¼°æµ‹ï¼Œè¯¯å·®ä¹Ÿä¼šæ›´å¤§ã€‚é‚£ä¹ˆä½¿ç”¨ä¸€ä¸ªç½‘ç»œå¯¹$Q,V$è¿›è¡Œä¼°è®¡ï¼Œå°±éœ€è¦è¿›è¡Œä¸€å®šçš„å˜æ¢ã€‚å®é™…ä¸Šåœ¨State $s$é‡‡å–Action $a$çš„Reward $r_t^n$ä¹Ÿæ˜¯ä¸€ä¸ªRandom Variableï¼Œæ˜¯ä¸ç¡®å®šçš„ã€‚ä¾‹å¦‚åœ¨ç©æ¸¸æˆæ—¶ï¼Œå¸Œæœ›ä½¿ç”¨ä¸€ä¸ªæŠ€èƒ½æ€æ­»æ•Œäººï¼ŒæŠ€èƒ½æ”¾å‡ºåç©¶ç«Ÿæ˜¯å¦èƒ½æ€æ­»æ•Œäººï¼Œåœ¨å†³å®šæ”¾æŠ€èƒ½æ—¶è¿˜æ˜¯ä¸ç¡®å®šçš„ã€‚å› æ­¤æœ‰$Q^{\pi}(s_t^n,a_t^n)=E[r_t^n+V^{\pi}(s_{t+1}^n)]$ï¼Œç„¶åå»æ‰æœŸæœ›ç¬¦å·ï¼Œè¿‘ä¼¼çš„è®¤ä¸ºç­‰å¼ä»ç„¶æˆç«‹ã€‚åˆ™å¯ä»¥å¾—åˆ°$Q^{\pi}(s_t^n,a_t^n)-V^{\pi}(s_t^n)=r_t^n+V^{\pi}(s_{t+1}^n)-V^{\pi}(s_t^n)$ï¼Œæ­¤æ—¶å…¬å¼ä¸­å­˜åœ¨ä¸€ä¸ªRandom Variable $r_t^n$ï¼Œç›¸æ¯”äº$G_t^n$ï¼Œ $r_t^n$æ›´ç¨³å®šä¸€äº›ï¼Œå› ä¸ºè¿™åªæ˜¯ä¸€æ­¥Actionçš„æ”¶ç›Šã€‚
   
     <img src="./image-20200826205703229.png" alt="image-20200826205703229" style="zoom: 50%;" />
     
   - å› ä¸º$r_t^n+V^{\pi}(s_{t+1}^n)-V^{\pi}(s_t^n)$è¢«ç§°ä¸ºAdvantage Functionï¼Œåˆæ˜¯ç»“åˆäº†Policy Gradientå’ŒQ-Learningï¼Œæ‰€ä»¥è¿™ç§æŠ€æœ¯è¢«ç§°ä¸ºAdvantage Actor-Criticã€‚
   
     <img src="./image-20200826211346264.png" alt="image-20200826211346264" style="zoom:50%;" />
     
   - Advantage Actor-Criticçš„è®­ç»ƒæŠ€å·§
   
     - Tip 1ï¼šAdvantage Actor-Criticéœ€è¦åšä¸¤ä»¶äº‹æƒ…ï¼Œç¬¬ä¸€ä»¶äº‹æ˜¯è¾“å…¥ä¸€ä¸ªState $s$ï¼Œè¾“å‡ºä¸€ä¸ªscalarï¼Œç”¨æ¥ä¼°è®¡$V^{\pi}(s)$ï¼›ç¬¬äºŒä»¶äº‹æ˜¯ä½¿ç”¨NNå­¦ä¹ ä¸€ä¸ªActor $\pi(s)$ï¼Œè¾“å…¥ä¸€ä¸ªState $s$ï¼Œè¾“å‡ºä¸€ä¸ªAction Distributionã€‚ä¸¤ä¸ªç½‘ç»œ$\pi(s)$å’Œ$V^{\pi}(s)$çš„å‰å‡ å±‚å…±äº«å‚æ•°ï¼ˆç»¿è‰²ï¼‰ï¼Œå…ˆæŠŠè¾“å…¥è½¬æ¢æˆä¸€äº›High-levelçš„ä¿¡æ¯ï¼Œç„¶ååœ¨åˆ†åˆ«å¤„ç†
     
     - Explorationçš„è¿‡ç¨‹ä»ç„¶æ˜¯å¾ˆé‡è¦çš„ï¼Œå› æ­¤å¯¹$\pi(s)$è¾“å‡ºçš„Action Distributionåšå‡ºä¸€äº›é™åˆ¶ï¼Œè¦æ±‚å…¶ä¿¡æ¯ç†µä¸èƒ½å¤ªå°ï¼Œå³ä¸åŒçš„Actionè¢«æ‰§è¡Œçš„å‡ ç‡å°½å¯èƒ½å¹³å‡ä¸€ç‚¹ï¼Œæœ‰åˆ©äºè¿›è¡Œæ›´å¤šçš„æ¢ç´¢ã€‚
     
       
   
2. <span name="1.2">Asynchronous Advantage Actor-Criticï¼ˆA3Cï¼‰</span>

   - A3CæŒ‡çš„æ˜¯Asynchronous Advantage Actor-Criticã€‚AsynchronousæŒ‡å­˜åœ¨ä¸€ä¸ªGlobalçš„Actorå’ŒCriticï¼Œæ¯ä¸€æ¬¡è¦å­¦ä¹ çš„æ—¶å€™ï¼Œå°±ä»Globalçš„Actorå’ŒCriticæ‹·è´ä¸€ç»„å‚æ•°ï¼Œè¿™æ ·å°±å¯ä»¥æ„å»ºå¤šä¸ªActorã€‚ç„¶åè®©Actorå’Œç¯å¢ƒè¿›è¡Œäº’åŠ¨ï¼Œè®¡ç®—éœ€è¦æ›´æ–°çš„å‚æ•°$\Delta \theta$å¹¶ä¼ å›Globalçš„Actorå’ŒCriticã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºåˆ›é€ å¤šä¸ªåˆ†èº«è¿›è¡Œå­¦ä¹ ï¼Œç„¶åæ±‡æ€»å­¦ä¹ ç»“æœã€‚

     <img src="./image-20200831222151172.png" alt="image-20200831222151172" style="zoom:50%;" />

   


#### <span name="2">2.Pathwise Derivative Policy Gradient</span>

1. <span name="2.1">å€Ÿé‰´GANçš„æ€æƒ³ä½¿ç”¨Actorè§£å†³Q-Learningçš„arg maxé—®é¢˜</span>

   - [David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller, â€œDeterministic Policy Gradient Algorithmsâ€, ICML, 2014]ã€ [Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, â€œCONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNINGâ€, ICLR, 2016]

   - Pathwise Derivative Policy Gradientå¯ä»¥è¢«è§†ä¸ºä¸€ç§è§£å†³Q-Learning Continuous Actionçš„æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥è§†ä¸ºä¸€ç§ç‰¹æ®Šçš„Actor-Criticæ–¹æ³•ã€‚ä¼ ç»Ÿçš„Actor-Criticä¼šä»¥Stateæˆ–State-Action Pairä½œä¸ºè¾“å…¥ï¼Œå¹¶ç»™å‡ºå½“å‰çš„Actionæ˜¯å¥½æ˜¯åã€‚ä½†æ˜¯Pathwise Derivative Policy Gradientä¸ä½†ä¼šç»™å‡ºå½“å‰Actionçš„å¥½ä¸åï¼ŒåŒæ—¶è¿˜ä¼šç»™å‡ºæœ€å¥½çš„Actionã€‚

   - Q-Learningä¸­å­˜åœ¨Continuous Actionè¾ƒéš¾å¤„ç†çš„é—®é¢˜ï¼Œè™½ç„¶å¯ä»¥è§£å†³ï¼Œä½†æ˜¯ä¸å®¹æ˜“ã€‚æ‰€ä»¥Pathwise Derivative Policy Gradientæå‡ºä½¿ç”¨Actorè§£æœ€ä¼˜åŒ–é—®é¢˜ï¼Œ$a=arg\ \max\limits_aQ(s,a)$ï¼Œå¯¹äºç»™å®šçš„State $s$è§£å‡ºå¯ä»¥è·å¾—æœ€å¤§$Q-Value$çš„Action $a$ã€‚è¿™ç§æƒ³æ³•ç±»ä¼¼äºGANï¼ŒGANä¸­Discriminatoræ— æ³•è§£å†³æœ€ä¼˜åŒ–çš„é—®é¢˜ï¼Œé‚£ä¹ˆå°±åœ¨è®­ç»ƒä¸€ä¸ªGeneratorè§£å†³è¯¥é—®é¢˜ã€‚

     <img src="./image-20200831231441819.png" alt="image-20200831231441819" style="zoom:50%;" />

   - å‡è®¾å·²ç»è®­ç»ƒäº†ä¸€ä¸ªCritic $Q^\pi$ï¼Œè¾“å…¥æ˜¯$s,a$ï¼Œè¾“å‡ºæ˜¯$Q^\pi(s,a)$ã€‚ç„¶ååœ¨è®­ç»ƒä¸€ä¸ªActorï¼Œå’ŒCriticç»„æˆä¸€ä¸ªå¤§çš„ç½‘ç»œã€‚Actorçš„ç›®æ ‡æ˜¯è¾“å‡ºåˆé€‚çš„Action $a$èƒ½å¤Ÿæœ€å¤§åŒ–Criticçš„è¾“å‡ºã€‚

     <img src="./image-20200831232427325.png" alt="image-20200831232427325" style="zoom:50%;" />

     <img src="./image-20200831232536643.png" alt="image-20200831232536643" style="zoom:50%;" />

   

2. <span name="2.2">Pathwise Derivative Policy Gradientç®—æ³•ä¼ªä»£ç </span>

   - Q-Learningçš„ç®—æ³•ä¼ªä»£ç å¦‚ä¸‹ï¼Œåœ¨ç¦»æ•£çš„æƒ…å†µä¸‹å·²çŸ¥State $s_t$ï¼ŒåŸºäº$Q-Function$çš„ç»“æ„é€‰æ‹©æœ€å¥½çš„Action $a_t$ã€‚

     <img src="./image-20200831233100469.png" alt="image-20200831233100469" style="zoom:50%;" />
     
   - Pathwise Derivative Policy Gradientçš„ç®—æ³•ä¼ªä»£ç å¦‚ä¸‹ï¼Œæœ‰å››å¤„å˜åŠ¨ã€‚ç¬¬ä¸€å¤„ä¸ºä¸åœ¨ä½¿ç”¨$Q-Function$å†³å®šå°†è¦æ‰§è¡Œçš„Actionï¼Œè€Œæ˜¯ä½¿ç”¨Actor $\pi$å†³å®šå°†è¦æ‰§è¡Œçš„Actionï¼›ç¬¬äºŒå¤„ä¸ºç›´æ¥å°†Actor $\pi$çš„ç»“æœä»£å…¥Target $y$ï¼Œ$\pi'$å’Œ$\pi$çš„åŒºåˆ«ç±»ä¼¼äºä¹‹å‰çš„$Q-function\ Q$å’Œ$target\ Q-function\ \hat{ğ‘„}$ ã€‚
   
     <img src="./image-20200831233139613.png" alt="image-20200831233139613" style="zoom:50%;" />
     
   - [David Pfau, Oriol Vinyals, â€œConnecting Generative Adversarial Networks and Actor-Critic Methodsâ€, arXiv preprint, 2016]ä¸­è§£é‡Šäº†Actor-Criticå’ŒGANä¹‹é—´çš„å…³ç³»ã€‚ä¸¤ä¸ªæ¨¡å‹çš„å…±åŒç‚¹ä¹‹ä¸€å°±æ˜¯éƒ½æ¯”è¾ƒéš¾è®­ç»ƒã€‚
   
     <img src="./image-20200831234517429.png" alt="image-20200831234517429" style="zoom:50%;" />
     
     
   
